{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6292c2c",
   "metadata": {},
   "source": [
    "# 02 — Join → Label → Clean → Export (Kepler DR25)\n",
    "\n",
    "**Goal:** Build the *frozen* tabular dataset for modeling.  \n",
    "This notebook turns raw Kepler CSVs into three artifacts:\n",
    "- `data/processed/features/table_v1.parquet`\n",
    "- `data/processed/labels/labels_v1.csv`\n",
    "- `data/processed/splits/split_v1.csv`\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs (from `data/raw/kepler/`)\n",
    "- `kepler_stellar_dr25.csv` — DR25 stellar catalog with astrophysical features and detectability/coverage fields (`teff, logg, feh, radius, mass, kepmag, rrmscdpp* (CDPP), dataspan, dutycycle, st_quarters`) plus per-star counts (`nconfp, nkoi, ntce`).\n",
    "- `kepler_stellar_dr25_supplement.csv` — supplemental (SPWG) refinements for a subset of stars.\n",
    "- `koi_cumulative.csv` and `koi_dr25.csv` — KOI tables (used to recompute/verify “candidate/confirmed” counts).\n",
    "\n",
    "---\n",
    "\n",
    "## What this notebook does\n",
    "\n",
    "1) **Lock paths to the repo root**  \n",
    "   Ensures reads/writes go to `data/...` at the repository root (not inside `notebooks/`). \n",
    "\n",
    "2) **Load and normalize keys**  \n",
    "   Reads the four CSVs and coerces `kepid` to integer consistently.  \n",
    "\n",
    "3) **Derive observing coverage (`nquarters`)**  \n",
    "   If only `st_quarters` is present, compute `nquarters` by counting 1’s in the bitstring. This lets us filter ambiguous negatives later. \n",
    "\n",
    "4) **Overlay Supplemental on DR25 (left-join by `kepid`)**  \n",
    "   For `teff, logg, feh, radius, mass`, prefer supplemental values when available; otherwise keep DR25. This yields a single master row per star.\n",
    "5) **Optional KOI aggregation for “lenient” positives**  \n",
    "   Aggregate KOIs with disposition ∈ {**CANDIDATE, CONFIRMED**} to `nkoi_pos` by `kepid` (source selectable via `USE_KOI_SOURCE = \"cumulative\" | \"dr25\" | \"none\"`). \n",
    "\n",
    "6) **Define two label policies (transparent & testable)**\n",
    "   - **Strict:** `label_strict = (nconfp ≥ 1)`  \n",
    "   - **Lenient:** `label_lenient = (nconfp ≥ 1) OR (nkoi_pos ≥ 1)`  \n",
    "   If KOI aggregation is disabled, `nkoi` from DR25 is used instead of `nkoi_pos`.  \n",
    "\n",
    "7) **Reduce label noise (coverage guardrails)**  \n",
    "   Keep *negatives* only if the star had adequate coverage:\n",
    "   - `nquarters ≥ MIN_QUARTERS` (default 8)  \n",
    "   - `dutycycle ≥ MIN_DUTY` (default 0.50)  \n",
    "   - `dataspan ≥` lower quartile (configurable via `DATASPAN_Q`)  \n",
    "   This avoids treating “not sufficiently observed” as “no planet detected.”  \n",
    "\n",
    "8) **Select/clean modeling features**  \n",
    "   Columns kept:  \n",
    "   `teff, logg, feh, radius, mass, kepmag, rrmscdpp03p0, rrmscdpp06p0, rrmscdpp12p0, dataspan, dutycycle, nquarters`.  \n",
    "   We coerce to numeric, **median-impute** remaining NaNs, and **winsorize** at 0.1%/99.9% to limit extreme tails. Two lightweight interactions are added: `feh_x_teff`, `radius_x_kepmag`. \n",
    "\n",
    "9) **Write artifacts**  \n",
    "   - **Features:** `features/table_v1.parquet` (one row per `kepid`)  \n",
    "   - **Labels:** `labels/labels_v1.csv` (raw counts + `label_strict`, `label_lenient`)  \n",
    "\n",
    "10) **Create stratified Train/Val/Test splits**  \n",
    "    Stratify by *(Teff bin × label_lenient)* to preserve stellar-type mix across splits.  \n",
    "    Save to `splits/split_v1.csv` with columns: `kepid, split ∈ {train,val,test}`.  \n",
    "\n",
    "---\n",
    "\n",
    "## Configuration knobs (top of the notebook)\n",
    "- `USE_KOI_SOURCE`: `\"cumulative\"` (default), `\"dr25\"`, or `\"none\"`  \n",
    "- `MIN_QUARTERS`, `MIN_DUTY`, `DATASPAN_Q`: coverage thresholds for filtering ambiguous negatives  \n",
    "- `RANDOM_SEED`: controls split reproducibility\n",
    "\n",
    "---\n",
    "\n",
    "## Why this design?\n",
    "\n",
    "- **Reproducibility:** a single, auditable place to build the dataset before any modeling.  \n",
    "- **Fair evaluation:** label policy is explicit; coverage filtering reduces false negatives; splits are stratified.  \n",
    "- **Flexibility:** KOI source and thresholds are toggles for ablation/sensitivity analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## Sanity checks to run after it finishes\n",
    "- Inspect class balance: `labels_v1.csv` should have rare positives (especially for `label_strict`).  \n",
    "- Confirm split counts and that each split contains a similar Teff distribution.  \n",
    "- Spot-check a few rows for realistic ranges (e.g., `dutycycle∈[0,1]`, CDPPs positive).\n",
    "\n",
    "*Outputs from this notebook are consumed by the preprocessing/EDA and modeling notebooks that follow.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cac5745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /Users/chrisjuarez/CPSC483_ML_Project\n",
      "True True True True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Find repo root (the folder that contains data/raw/kepler)\n",
    "ROOT = Path.cwd()\n",
    "for up in [ROOT, *ROOT.parents]:\n",
    "    if (up / \"data\" / \"raw\" / \"kepler\").exists():\n",
    "        ROOT = up\n",
    "        break\n",
    "else:\n",
    "    raise FileNotFoundError(\"Could not locate repo root with data/raw/kepler\")\n",
    "\n",
    "RAW = ROOT / \"data\" / \"raw\" / \"kepler\"\n",
    "OUT = ROOT / \"data\" / \"processed\"\n",
    "\n",
    "# Read files\n",
    "df_star = pd.read_csv(RAW / \"kepler_stellar_dr25.csv\")\n",
    "df_sup  = pd.read_csv(RAW / \"kepler_stellar_dr25_supplement.csv\")\n",
    "df_kcum = pd.read_csv(RAW / \"koi_cumulative.csv\")\n",
    "df_kdr  = pd.read_csv(RAW / \"koi_dr25.csv\")\n",
    "\n",
    "# sanity\n",
    "print(\"ROOT:\", ROOT)\n",
    "print((RAW/\"kepler_stellar_dr25.csv\").exists(),\n",
    "      (RAW/\"kepler_stellar_dr25_supplement.csv\").exists(),\n",
    "      (RAW/\"koi_cumulative.csv\").exists(),\n",
    "      (RAW/\"koi_dr25.csv\").exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5472ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /Users/chrisjuarez/CPSC483_ML_Project/notebooks\n",
      "Configuration: KOI source=cumulative, Min quarters=8, Min duty=0.5\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "USE_KOI_SOURCE = \"cumulative\"  # options: \"cumulative\", \"dr25\", \"none\"\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Coverage filtering thresholds (critical for data quality)\n",
    "MIN_QUARTERS = 8      # At least 2 years of observation\n",
    "MIN_DUTY = 0.50       # At least 50% duty cycle\n",
    "DATASPAN_Q = 0.25     # Use lower quartile as minimum\n",
    "\n",
    "print(f\"Working directory: {Path.cwd()}\")\n",
    "print(f\"Configuration: KOI source={USE_KOI_SOURCE}, Min quarters={MIN_QUARTERS}, Min duty={MIN_DUTY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5affdd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data shapes:\n",
      "  DR25 stellar: (200038, 16)\n",
      "  Supplement: (197096, 6)\n",
      "  KOI cumulative: (9564, 5)\n",
      "  KOI DR25: (8054, 5)\n"
     ]
    }
   ],
   "source": [
    "df_star = pd.read_csv(RAW / \"kepler_stellar_dr25.csv\")\n",
    "df_sup  = pd.read_csv(RAW / \"kepler_stellar_dr25_supplement.csv\")\n",
    "df_koi_cum  = pd.read_csv(RAW / \"koi_cumulative.csv\")\n",
    "df_koi_dr25 = pd.read_csv(RAW / \"koi_dr25.csv\")\n",
    "print(f\"Loaded data shapes:\")\n",
    "print(f\"  DR25 stellar: {df_star.shape}\")\n",
    "print(f\"  Supplement: {df_sup.shape}\")\n",
    "print(f\"  KOI cumulative: {df_koi_cum.shape}\")\n",
    "print(f\"  KOI DR25: {df_koi_dr25.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20f2808c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating nquarters from st_quarters...\n",
      "nquarters statistics:\n",
      "  Mean: 14.07\n",
      "  Min: 0, Max: 17\n",
      "  Zero values: 278\n",
      "\n",
      "Sample of calculated nquarters:\n",
      "      kepid        st_quarters  nquarters\n",
      "0  10000785   1111111111111111         16\n",
      "1  10000797  11111111111111111         17\n",
      "2  10000800  11111111111111111         17\n",
      "3  10000823  11111111111111111         17\n",
      "4  10000827  11111111111111111         17\n"
     ]
    }
   ],
   "source": [
    "def count_ones(s):\n",
    "    \"\"\"Count '1's in the quarters string to get number of observed quarters.\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return 0\n",
    "    # Convert to string and count 1s\n",
    "    s_str = str(s).strip().strip('\"')\n",
    "    return s_str.count('1')\n",
    "\n",
    "# Calculate nquarters BEFORE any type conversions\n",
    "if \"st_quarters\" in df_star.columns:\n",
    "    print(\"Calculating nquarters from st_quarters...\")\n",
    "    df_star[\"nquarters\"] = df_star[\"st_quarters\"].apply(count_ones)\n",
    "    \n",
    "    # Verify the calculation worked\n",
    "    print(f\"nquarters statistics:\")\n",
    "    print(f\"  Mean: {df_star['nquarters'].mean():.2f}\")\n",
    "    print(f\"  Min: {df_star['nquarters'].min()}, Max: {df_star['nquarters'].max()}\")\n",
    "    print(f\"  Zero values: {(df_star['nquarters'] == 0).sum()}\")\n",
    "    \n",
    "    # Show sample to verify\n",
    "    sample_idx = df_star['nquarters'] > 0\n",
    "    if sample_idx.any():\n",
    "        print(\"\\nSample of calculated nquarters:\")\n",
    "        print(df_star.loc[sample_idx.head(5).index, ['kepid', 'st_quarters', 'nquarters']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "462981f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key normalization complete\n"
     ]
    }
   ],
   "source": [
    "# Ensure kepid is integer in all dataframes\n",
    "for df in [df_star, df_sup, df_koi_cum, df_koi_dr25]:\n",
    "    if \"kepid\" in df.columns:\n",
    "        df[\"kepid\"] = pd.to_numeric(df[\"kepid\"], errors=\"coerce\")\n",
    "        df = df.dropna(subset=[\"kepid\"])\n",
    "        df[\"kepid\"] = df[\"kepid\"].astype(\"int64\")\n",
    "\n",
    "print(f\"Key normalization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d39ec6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset shape: (200038, 17)\n",
      "Stars with supplement data: 197096\n"
     ]
    }
   ],
   "source": [
    "# Columns to overlay from supplement\n",
    "overlay_cols = [\"teff\", \"logg\", \"feh\", \"radius\", \"mass\"]\n",
    "\n",
    "# Merge supplement data\n",
    "df = df_star.merge(\n",
    "    df_sup[[\"kepid\"] + overlay_cols], \n",
    "    on=\"kepid\", \n",
    "    how=\"left\", \n",
    "    suffixes=(\"\", \"_sup\")\n",
    ")\n",
    "\n",
    "# Prefer supplement values when available\n",
    "for col in overlay_cols:\n",
    "    if f\"{col}_sup\" in df.columns:\n",
    "        df[col] = df[f\"{col}_sup\"].combine_first(df[col])\n",
    "        df.drop(columns=[f\"{col}_sup\"], inplace=True)\n",
    "\n",
    "print(f\"Merged dataset shape: {df.shape}\")\n",
    "print(f\"Stars with supplement data: {df_sup['kepid'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "906d6372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KOI aggregation using source: cumulative\n",
      "Stars with positive KOIs: 3611\n"
     ]
    }
   ],
   "source": [
    "def koi_pos_counts(koi_df):\n",
    "    \"\"\"Count positive KOI dispositions per star.\"\"\"\n",
    "    if koi_df.empty:\n",
    "        return pd.DataFrame(columns=[\"kepid\", \"nkoi_pos\"])\n",
    "    \n",
    "    # Find disposition column\n",
    "    disp_col = None\n",
    "    for col in [\"koi_disposition\", \"disposition\", \"koi_pdisposition\"]:\n",
    "        if col in koi_df.columns:\n",
    "            disp_col = col\n",
    "            break\n",
    "    \n",
    "    if disp_col:\n",
    "        koi = koi_df.copy()\n",
    "        koi[disp_col] = koi[disp_col].astype(str).str.upper()\n",
    "        koi = koi[koi[disp_col].isin([\"CANDIDATE\", \"CONFIRMED\"])]\n",
    "        return koi.groupby(\"kepid\").size().rename(\"nkoi_pos\").reset_index()\n",
    "    else:\n",
    "        return pd.DataFrame(columns=[\"kepid\", \"nkoi_pos\"])\n",
    "\n",
    "# Select KOI source and aggregate\n",
    "koi_sources = {\n",
    "    \"cumulative\": df_koi_cum,\n",
    "    \"dr25\": df_koi_dr25,\n",
    "    \"none\": pd.DataFrame()\n",
    "}\n",
    "\n",
    "koi_used = koi_sources.get(USE_KOI_SOURCE, pd.DataFrame())\n",
    "koi_counts = koi_pos_counts(koi_used)\n",
    "\n",
    "# Merge KOI counts\n",
    "df = df.merge(koi_counts, on=\"kepid\", how=\"left\")\n",
    "df[\"nkoi_pos\"] = df[\"nkoi_pos\"].fillna(0).astype(int)\n",
    "\n",
    "print(f\"KOI aggregation using source: {USE_KOI_SOURCE}\")\n",
    "print(f\"Stars with positive KOIs: {(df['nkoi_pos'] > 0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe17c180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label statistics:\n",
      "  Strict positives: 1982 (0.99%)\n",
      "  Lenient positives: 3628 (1.81%)\n"
     ]
    }
   ],
   "source": [
    "# Ensure count columns exist\n",
    "for col in [\"nconfp\", \"nkoi\", \"ntce\"]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = 0\n",
    "\n",
    "# Create two label policies\n",
    "df[\"label_strict\"] = (df[\"nconfp\"] >= 1).astype(int)\n",
    "df[\"label_lenient\"] = ((df[\"nconfp\"] >= 1) | (df[\"nkoi_pos\"] >= 1)).astype(int)\n",
    "\n",
    "# If KOI aggregation disabled, use nkoi from DR25\n",
    "if USE_KOI_SOURCE == \"none\":\n",
    "    df[\"label_lenient\"] = ((df[\"nconfp\"] >= 1) | (df[\"nkoi\"] >= 1)).astype(int)\n",
    "\n",
    "print(f\"Label statistics:\")\n",
    "print(f\"  Strict positives: {df['label_strict'].sum()} ({df['label_strict'].mean()*100:.2f}%)\")\n",
    "print(f\"  Lenient positives: {df['label_lenient'].sum()} ({df['label_lenient'].mean()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "951247a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage statistics before filtering:\n",
      "  Total stars: 200038\n",
      "  Good coverage: 150331 (75.2%)\n",
      "  Poor coverage: 49707 (24.8%)\n",
      "\n",
      "After coverage filtering:\n",
      "  Total stars: 150762 (removed 49276)\n",
      "  Positives kept: 3628 (100% retained)\n",
      "  Negatives kept: 147134\n",
      "  New positive rate: 2.41%\n"
     ]
    }
   ],
   "source": [
    "# Calculate dataspan threshold\n",
    "dataspan_threshold = df[\"dataspan\"].quantile(DATASPAN_Q) if \"dataspan\" in df.columns else 0\n",
    "\n",
    "# Define good coverage\n",
    "good_coverage = (\n",
    "    (df[\"nquarters\"] >= MIN_QUARTERS) & \n",
    "    (df[\"dutycycle\"] >= MIN_DUTY) & \n",
    "    (df[\"dataspan\"] >= dataspan_threshold)\n",
    ")\n",
    "\n",
    "print(f\"Coverage statistics before filtering:\")\n",
    "print(f\"  Total stars: {len(df)}\")\n",
    "print(f\"  Good coverage: {good_coverage.sum()} ({good_coverage.mean()*100:.1f}%)\")\n",
    "print(f\"  Poor coverage: {(~good_coverage).sum()} ({(~good_coverage).mean()*100:.1f}%)\")\n",
    "\n",
    "# CRITICAL: Keep ALL positives, filter only poorly-observed negatives\n",
    "df_filtered = df[\n",
    "    (df[\"label_lenient\"] == 1) |  # Keep all positives\n",
    "    (good_coverage)                # Keep only well-observed negatives\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nAfter coverage filtering:\")\n",
    "print(f\"  Total stars: {len(df_filtered)} (removed {len(df) - len(df_filtered)})\")\n",
    "print(f\"  Positives kept: {df_filtered['label_lenient'].sum()} (100% retained)\")\n",
    "print(f\"  Negatives kept: {(df_filtered['label_lenient'] == 0).sum()}\")\n",
    "print(f\"  New positive rate: {df_filtered['label_lenient'].mean()*100:.2f}%\")\n",
    "\n",
    "# Replace df with filtered version\n",
    "df = df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0726e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled mass missing values with median: 0.987\n",
      "Filled kepmag missing values with median: 14.481\n",
      "Filled rrmscdpp03p0 missing values with median: 144.674\n",
      "Filled rrmscdpp06p0 missing values with median: 117.165\n",
      "Filled rrmscdpp12p0 missing values with median: 97.240\n",
      "Filled dataspan missing values with median: 1458.931\n",
      "Filled dutycycle missing values with median: 0.874\n",
      "Basic features processed: 12\n"
     ]
    }
   ],
   "source": [
    "# Basic features to keep\n",
    "feature_cols = [\n",
    "    \"teff\", \"logg\", \"feh\", \"radius\", \"mass\", \"kepmag\",\n",
    "    \"rrmscdpp03p0\", \"rrmscdpp06p0\", \"rrmscdpp12p0\",\n",
    "    \"dataspan\", \"dutycycle\", \"nquarters\"\n",
    "]\n",
    "\n",
    "# Ensure all features exist and clean\n",
    "for col in feature_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# Fill missing values with median\n",
    "for col in feature_cols:\n",
    "    if col in df.columns and df[col].isna().any():\n",
    "        median_val = df[col].median()\n",
    "        df[col] = df[col].fillna(median_val)\n",
    "        print(f\"Filled {col} missing values with median: {median_val:.3f}\")\n",
    "\n",
    "# Winsorize extreme values\n",
    "for col in feature_cols:\n",
    "    if col in df.columns:\n",
    "        lo, hi = df[col].quantile([0.001, 0.999])\n",
    "        df[col] = df[col].clip(lo, hi)\n",
    "\n",
    "print(f\"Basic features processed: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dbbf9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating astrophysics-informed features...\n",
      "Total features created: 23\n",
      "New features: ['feh_x_teff', 'radius_x_kepmag', 'transit_prob', 'stellar_density', 'noise_star_ratio', 'noise_consistency', 'obs_quality', 'detection_eff', 'rrmscdpp03p0_log', 'rrmscdpp06p0_log', 'rrmscdpp12p0_log']\n"
     ]
    }
   ],
   "source": [
    "# Create domain-specific features\n",
    "print(\"Creating astrophysics-informed features...\")\n",
    "\n",
    "# 1. Basic interactions (original)\n",
    "if {\"feh\", \"teff\"}.issubset(df.columns):\n",
    "    df[\"feh_x_teff\"] = df[\"feh\"] * df[\"teff\"]\n",
    "\n",
    "if {\"radius\", \"kepmag\"}.issubset(df.columns):\n",
    "    df[\"radius_x_kepmag\"] = df[\"radius\"] * df[\"kepmag\"]\n",
    "\n",
    "# 2. Transit probability indicator\n",
    "if {\"radius\", \"teff\"}.issubset(df.columns):\n",
    "    df[\"transit_prob\"] = df[\"radius\"] / (df[\"radius\"] + 0.00465 * df[\"teff\"])\n",
    "\n",
    "# 3. Stellar density proxy\n",
    "if {\"mass\", \"radius\"}.issubset(df.columns):\n",
    "    df[\"stellar_density\"] = df[\"mass\"] / (df[\"radius\"].clip(lower=0.1) ** 3)\n",
    "\n",
    "# 4. Noise ratios (detectability)\n",
    "if {\"rrmscdpp06p0\", \"kepmag\"}.issubset(df.columns):\n",
    "    df[\"noise_star_ratio\"] = df[\"rrmscdpp06p0\"] / df[\"kepmag\"].clip(lower=1)\n",
    "\n",
    "# 5. Noise consistency (activity proxy)\n",
    "if {\"rrmscdpp03p0\", \"rrmscdpp12p0\"}.issubset(df.columns):\n",
    "    df[\"noise_consistency\"] = df[\"rrmscdpp03p0\"] / df[\"rrmscdpp12p0\"].clip(lower=1)\n",
    "\n",
    "# 6. Observation quality score\n",
    "if {\"dutycycle\", \"nquarters\", \"dataspan\"}.issubset(df.columns):\n",
    "    df[\"obs_quality\"] = (df[\"dutycycle\"] * df[\"nquarters\"]) / df[\"dataspan\"].clip(lower=1)\n",
    "\n",
    "# 7. Detection efficiency\n",
    "if all(col in df.columns for col in [\"radius\", \"kepmag\", \"rrmscdpp06p0\"]):\n",
    "    df[\"detection_eff\"] = (df[\"radius\"] ** 2) / (\n",
    "        df[\"kepmag\"].clip(lower=1) * df[\"rrmscdpp06p0\"].clip(lower=1)\n",
    "    )\n",
    "\n",
    "# 8. Log transforms for highly skewed CDPP features\n",
    "for col in [\"rrmscdpp03p0\", \"rrmscdpp06p0\", \"rrmscdpp12p0\"]:\n",
    "    if col in df.columns:\n",
    "        df[f\"{col}_log\"] = np.log1p(df[col])\n",
    "\n",
    "# Get all feature columns\n",
    "all_feature_cols = [col for col in df.columns if col not in \n",
    "                   [\"kepid\", \"label_strict\", \"label_lenient\", \"nconfp\", \"nkoi\", \n",
    "                    \"ntce\", \"nkoi_pos\", \"st_quarters\"]]\n",
    "\n",
    "print(f\"Total features created: {len(all_feature_cols)}\")\n",
    "print(f\"New features: {[col for col in all_feature_cols if col not in feature_cols]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "180ef060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features: 23 columns\n",
      "Saved labels for 150762 stars\n"
     ]
    }
   ],
   "source": [
    "# Prepare feature and label dataframes\n",
    "feat_cols = [\"kepid\"] + all_feature_cols\n",
    "lab_cols = [\"kepid\", \"nconfp\", \"nkoi\", \"ntce\", \"label_strict\", \"label_lenient\"]\n",
    "\n",
    "# Save features and labels\n",
    "df[feat_cols].to_parquet(OUT/\"features/table_v1.parquet\", index=False)\n",
    "df[lab_cols].to_csv(OUT/\"labels/labels_v1.csv\", index=False)\n",
    "\n",
    "print(f\"Saved features: {len(all_feature_cols)} columns\")\n",
    "print(f\"Saved labels for {len(df)} stars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e930715f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split statistics:\n",
      "  train: 105532 samples, 2540 positives (2.41%)\n",
      "  val  :  15077 samples,  363 positives (2.41%)\n",
      "  test :  30153 samples,  725 positives (2.40%)\n",
      "\n",
      "Total samples: 150762\n",
      "Overall positive rate: 2.41%\n",
      "✅ Splits are well-balanced\n"
     ]
    }
   ],
   "source": [
    "def teff_bin(x):\n",
    "    \"\"\"Bin effective temperature for stratification.\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return -1\n",
    "    elif x < 3800:\n",
    "        return 0  # M dwarfs\n",
    "    elif x < 5200:\n",
    "        return 1  # K dwarfs  \n",
    "    elif x < 6000:\n",
    "        return 2  # G dwarfs\n",
    "    else:\n",
    "        return 3  # F dwarfs and hotter\n",
    "\n",
    "# Reset index to ensure continuous indices for splitting\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Prepare stratification\n",
    "df[\"teff_bin\"] = df[\"teff\"].apply(teff_bin) if \"teff\" in df.columns else 0\n",
    "df[\"strata\"] = df[\"label_lenient\"].astype(str) + \"_\" + df[\"teff_bin\"].astype(str)\n",
    "\n",
    "# Initialize split column\n",
    "df[\"split\"] = \"train\"\n",
    "\n",
    "# First split: 80% train+val, 20% test\n",
    "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.20, random_state=RANDOM_SEED)\n",
    "train_val_idx, test_idx = next(sss1.split(df.index, df[\"strata\"]))\n",
    "df.loc[test_idx, \"split\"] = \"test\"\n",
    "\n",
    "# Second split: From the 80%, take 10% for validation (10/80 = 0.125)\n",
    "train_val_df = df.loc[train_val_idx]\n",
    "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.125, random_state=RANDOM_SEED+1)\n",
    "train_idx, val_idx = next(sss2.split(train_val_df.index, train_val_df[\"strata\"]))\n",
    "df.loc[train_val_df.iloc[val_idx].index, \"split\"] = \"val\"\n",
    "\n",
    "# Save splits (drop temporary columns)\n",
    "df[[\"kepid\", \"split\"]].to_csv(OUT/\"splits/split_v1.csv\", index=False)\n",
    "\n",
    "# Print split statistics\n",
    "print(\"\\nSplit statistics:\")\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    subset = df[df[\"split\"] == split]\n",
    "    n_samples = len(subset)\n",
    "    n_positive = subset[\"label_lenient\"].sum()\n",
    "    pos_rate = subset[\"label_lenient\"].mean()\n",
    "    print(f\"  {split:5s}: {n_samples:6d} samples, {n_positive:4d} positives ({pos_rate:.2%})\")\n",
    "\n",
    "print(f\"\\nTotal samples: {len(df)}\")\n",
    "print(f\"Overall positive rate: {df['label_lenient'].mean():.2%}\")\n",
    "\n",
    "# Verify stratification worked\n",
    "rates = df.groupby(\"split\")[\"label_lenient\"].mean()\n",
    "if rates.std() < 0.001:\n",
    "    print(\"✅ Splits are well-balanced\")\n",
    "else:\n",
    "    print(f\"⚠️ Warning: Positive rates vary by {rates.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1d9a0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA PREPARATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Dataset Summary:\n",
      "- Total stars processed: 150,762\n",
      "- Features created: 23\n",
      "- Positive examples: 3,628 (2.41%)\n",
      "- Coverage filtered negatives: 0\n",
      "\n",
      " Files Created:\n",
      "- features/table_v1.parquet (23 features)\n",
      "- labels/labels_v1.csv (labels and counts)\n",
      "- splits/split_v1.csv (train/val/test assignments)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA PREPARATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\"\"\n",
    "Dataset Summary:\n",
    "- Total stars processed: {len(df):,}\n",
    "- Features created: {len(all_feature_cols)}\n",
    "- Positive examples: {df['label_lenient'].sum():,} ({df['label_lenient'].mean()*100:.2f}%)\n",
    "- Coverage filtered negatives: {len(df) - len(df_filtered) if 'df_filtered' in locals() else 0:,}\n",
    "\n",
    " Files Created:\n",
    "- features/table_v1.parquet ({len(all_feature_cols)} features)\n",
    "- labels/labels_v1.csv (labels and counts)\n",
    "- splits/split_v1.csv (train/val/test assignments)\n",
    "\"\"\")\n",
    "\n",
    "# Save processing metadata\n",
    "metadata = {\n",
    "    \"processing_date\": pd.Timestamp.now().isoformat(),\n",
    "    \"n_features\": len(all_feature_cols),\n",
    "    \"n_samples\": len(df),\n",
    "    \"positive_rate\": df['label_lenient'].mean(),\n",
    "    \"coverage_thresholds\": {\n",
    "        \"min_quarters\": MIN_QUARTERS,\n",
    "        \"min_duty\": MIN_DUTY,\n",
    "        \"dataspan_quantile\": DATASPAN_Q\n",
    "    }\n",
    "}\n",
    "\n",
    "pd.Series(metadata).to_csv(OUT/\"processing_metadata.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
