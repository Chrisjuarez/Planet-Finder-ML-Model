{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23f2caad",
   "metadata": {},
   "source": [
    "# 03 — Prepare Data \n",
    "\n",
    "**Purpose.** This notebook performs **data preprocessing only**. It takes the joined Kepler tables we built earlier and produces cleaned, standardized feature matrices and a reusable **preprocessing transformer**.\n",
    "\n",
    "## TL;DR\n",
    "- **Input:** `data/processed/features/table_v1.parquet`, `labels/labels_v1.csv`, `splits/split_v1.csv`\n",
    "- **Process (train-only fit to avoid leakage):**\n",
    "  1) **Impute** missing numeric values (median) and add missingness indicators  \n",
    "  2) **Winsorize** extreme tails (clip at the 0.1% / 99.9% quantiles)  \n",
    "  3) **Power-transform** skewed distributions (Yeo–Johnson; safe with zeros/negatives)  \n",
    "  4) **Standardize** features (mean 0, std 1) for linear models  \n",
    "- **Output:** \n",
    "  - `artifacts/preprocess_v1.pkl` — fitted preprocessing pipeline  \n",
    "  - Clean matrices: `data/processed/features_proc/X_{train,val,test}.parquet`  \n",
    "  - Targets/IDs: `data/processed/features_proc/y_{train,val,test}.csv`\n",
    "\n",
    "## Why this structure?\n",
    "This follows the Appendix B “Explore/Prepare the Data” guidance: work on **copies** of the data, **write functions/pipelines** for every transform so they are repeatable, fix/guard **outliers**, **impute** missing values, and **scale** features for algorithms that need it. We **fit the transformer on the train split only** and apply it to val/test to avoid leakage (cleaning choices must not “peek” at held-out data). \n",
    "\n",
    "## Inputs (what each file contains)\n",
    "- **`table_v1.parquet`** — one row per `kepid` with astrophysical + detectability features already joined and cleaned at a basic level.  \n",
    "- **`labels_v1.csv`** — `label_strict` and `label_lenient` built from DR25 counts and (optionally) KOI dispositions.  \n",
    "- **`split_v1.csv`** — stratified `train/val/test` tags by (Teff-bin × label).\n",
    "\n",
    "## Transform details (with rationale)\n",
    "- **Median imputation (+ indicator columns):** robust, simple, and lets models learn if “missingness” is predictive.  \n",
    "- **Winsorization (0.1%/99.9%):** tames extreme tails/outliers without dropping rows.  \n",
    "- **Yeo–Johnson power transform:** reduces skew for many features (e.g., CDPPs, radius) while handling zeros.  \n",
    "- **Standardization:** brings features onto comparable scales—important for Logistic Regression and distance-based methods; harmless for trees.\n",
    "\n",
    "All steps are packaged in a single **`sklearn` `ColumnTransformer` pipeline** so we can: (i) reuse it across notebooks, (ii) reproduce results easily, and (iii) treat preprocessing as hyperparameters later if needed.\n",
    "\n",
    "## Leakage policy\n",
    "- **Fit** the transformer on **train only**.  \n",
    "- **Transform** train/val/test using that frozen transformer.  \n",
    "This guarantees validation/test statistics reflect generalization, not “cleaning with future knowledge.”\n",
    "\n",
    "## Artifacts & how downstream code uses them\n",
    "- **`preprocess_v1.pkl`**: load it and call `.transform(X)` in any training notebook.  \n",
    "- **`X_*.parquet` / `y_*.csv`**: ready-to-fit matrices/labels for quick baselines.\n",
    "\n",
    "## Sanity checks we run here\n",
    "- Shapes of `X/y` for each split  \n",
    "- Positive class rate per split (should be **much less than 1.0**; if it’s 1.0 you filtered out all negatives—see Troubleshooting)\n",
    "\n",
    "## Troubleshooting\n",
    "- **All labels are 1 (positive rate ~1.0):** coverage filter in the *previous* notebook likely removed every negative. Loosen it and rebuild `table_v1.parquet`:\n",
    "  - Try `MIN_QUARTERS = 4`, `MIN_DUTY = 0.2`, `DATASPAN_Q = 0.05`, then re-export.  \n",
    "- **Feature count changed unexpectedly:** a column may be entirely missing/constant; check the attribute table in the EDA notebook and update the feature list if needed.\n",
    "\n",
    "## Reproducibility\n",
    "- This notebook is deterministic (fixed seeds upstream) and writes a mini “data card” with timestamp, target, and transform list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24d72e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(       kepid    teff   logg   feh  radius   mass  kepmag  rrmscdpp03p0  \\\n",
       " 0   10000785  5333.0  4.616 -1.00   0.650  0.635  15.749       445.410   \n",
       " 1   10000797  6289.0  4.270 -0.44   1.195  0.968  13.994        80.767   \n",
       " 2   10000800  5692.0  4.547 -0.04   0.866  0.965  15.379       226.348   \n",
       " 3   10000823  6580.0  4.377 -0.16   1.169  1.191  15.558       181.468   \n",
       " 4   10000827  5648.0  4.559 -0.10   0.841  0.939  14.841       124.834   \n",
       " 5   10000876  5249.0  4.410  0.18   0.953  0.849  14.458       104.839   \n",
       " 6   10000939  4312.0  4.663 -0.50   0.579  0.564  15.939       312.067   \n",
       " 7   10000941  5115.0  4.477  0.08   0.854  0.798  13.632        86.826   \n",
       " 8   10000962  5496.0  4.592 -0.24   0.776  0.869  14.574       109.243   \n",
       " 9   10000976  5629.0  4.546  0.04   0.870  0.972  14.586       133.739   \n",
       " 10  10000981  5107.0  3.490 -0.58   2.706  0.825  13.233       222.827   \n",
       " 11  10001000  5009.0  4.516  0.02   0.801  0.768  15.229       194.697   \n",
       " 12  10001002  6409.0  4.360 -0.46   1.092  0.997  13.453        84.453   \n",
       " 13  10001045  6591.0  3.983 -0.44   1.817  1.157  13.694        70.036   \n",
       " 14  10001085  6108.0  4.466  0.07   1.030  1.132  15.925       268.044   \n",
       " 15  10001116  5475.0  4.562 -0.12   0.811  0.876  14.002       107.188   \n",
       " 16  10001129  5064.0  4.613 -0.08   0.733  0.823  15.663       254.322   \n",
       " 17  10001142  5289.0  4.600 -0.16   0.760  0.848  15.844       277.656   \n",
       " 18  10001145  7865.0  3.941 -0.32   2.322  1.716  11.723       341.288   \n",
       " 19  10001154  4583.0  2.334 -0.20  11.644  1.067   8.801        29.695   \n",
       " \n",
       "     rrmscdpp06p0  rrmscdpp12p0  ...  detection_eff  rrmscdpp03p0_log  \\\n",
       " 0        499.980       589.300  ...       0.000054          6.101238   \n",
       " 1         60.264        45.939  ...       0.001693          4.403874   \n",
       " 2        184.595       158.220  ...       0.000264          5.426482   \n",
       " 3        148.879       132.140  ...       0.000590          5.206575   \n",
       " 4         92.096        67.532  ...       0.000517          4.834964   \n",
       " 5         78.884        62.140  ...       0.000796          4.661919   \n",
       " 6        249.728       210.126  ...       0.000084          5.746417   \n",
       " 7         80.053        88.747  ...       0.000668          4.475358   \n",
       " 8         80.700        60.509  ...       0.000512          4.702687   \n",
       " 9        108.927        96.324  ...       0.000476          4.903340   \n",
       " 10       225.402       216.257  ...       0.002455          5.410873   \n",
       " 11       177.442       180.949  ...       0.000237          5.276568   \n",
       " 12        80.216        88.485  ...       0.001105          4.447967   \n",
       " 13        51.995        38.708  ...       0.004637          4.263187   \n",
       " 14       201.735       154.433  ...       0.000330          5.594875   \n",
       " 15        99.645       110.749  ...       0.000471          4.683870   \n",
       " 16       208.735       194.016  ...       0.000164          5.542525   \n",
       " 17       220.717       180.449  ...       0.000165          5.629978   \n",
       " 18       274.728       201.182  ...       0.001674          5.835652   \n",
       " 19        35.584        42.240  ...       0.432930          3.424100   \n",
       " \n",
       "     rrmscdpp06p0_log  rrmscdpp12p0_log  nconfp  nkoi  ntce  label_strict  \\\n",
       " 0           6.216566          6.380631       0     0     2             0   \n",
       " 1           4.115192          3.848849       0     0     0             0   \n",
       " 2           5.223567          5.070287       0     0     0             0   \n",
       " 3           5.009828          4.891401       0     0     0             0   \n",
       " 4           4.533631          4.227301       0     0     0             0   \n",
       " 5           4.380576          4.145354       0     0     0             0   \n",
       " 6           5.524369          5.352455       0     0     0             0   \n",
       " 7           4.395103          4.496995       1     2     2             1   \n",
       " 8           4.403054          4.119184       0     0     0             0   \n",
       " 9           4.699817          4.578046       0     0     0             0   \n",
       " 10          5.422312          5.381081       0     0     0             0   \n",
       " 11          5.184264          5.203726       0     0     5             0   \n",
       " 12          4.397112          4.494071       0     0     0             0   \n",
       " 13          3.970198          3.681553       0     0     0             0   \n",
       " 14          5.311900          5.046215       0     0     0             0   \n",
       " 15          4.611599          4.716255       0     0     0             0   \n",
       " 16          5.345845          5.273082       0     0     0             0   \n",
       " 17          5.401402          5.200975       0     0     0             0   \n",
       " 18          5.619415          5.309168       0     0     2             0   \n",
       " 19          3.599611          3.766766       0     0     0             0   \n",
       " \n",
       "     label_lenient  split  \n",
       " 0               0  train  \n",
       " 1               0  train  \n",
       " 2               0   test  \n",
       " 3               0    val  \n",
       " 4               0  train  \n",
       " 5               0  train  \n",
       " 6               0  train  \n",
       " 7               1    val  \n",
       " 8               0  train  \n",
       " 9               0  train  \n",
       " 10              0   test  \n",
       " 11              0  train  \n",
       " 12              0  train  \n",
       " 13              0   test  \n",
       " 14              0  train  \n",
       " 15              0  train  \n",
       " 16              0  train  \n",
       " 17              0  train  \n",
       " 18              0  train  \n",
       " 19              0    val  \n",
       " \n",
       " [20 rows x 30 columns],\n",
       " 150762,\n",
       " ['teff', 'logg', 'feh', 'radius', 'mass', 'kepmag'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "\n",
    "ROOT = Path(\"/Users/chrisjuarez/CPSC483_ML_Project\")  \n",
    "X_PATH = ROOT/\"data/processed/features/table_v1.parquet\"\n",
    "Y_PATH = ROOT/\"data/processed/labels/labels_v1.csv\"\n",
    "SPLIT  = ROOT/\"data/processed/splits/split_v1.csv\"\n",
    "\n",
    "dfX = pd.read_parquet(X_PATH)\n",
    "dfy = pd.read_csv(Y_PATH)\n",
    "spl = pd.read_csv(SPLIT)\n",
    "df  = dfX.merge(dfy, on=\"kepid\").merge(spl, on=\"kepid\")\n",
    "\n",
    "FEATURES = [c for c in dfX.columns if c != \"kepid\"]\n",
    "TARGET   = \"label_lenient\"     # switch to label_strict if you prefer\n",
    "\n",
    "df.head(20), len(df), FEATURES[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e14bfe23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/numpy/core/_methods.py:176: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/numpy/core/_methods.py:187: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Saved artifacts/preprocess_v1.pkl'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np, pandas as pd, joblib, os\n",
    "\n",
    "# Winsorizer to tame extreme tails (robust \"clip\"); appendix: fix/remove outliers. \n",
    "# (Keeps copies; originals remain in data/processed.)   [oai_citation:4‡Appendix B Machine Learning Project Checklist.pdf](file-service://file-QnwgSaxpCdwK5LF6e2wnim)\n",
    "class Winsorize(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, p_lo=0.001, p_hi=0.999):\n",
    "        self.p_lo=p_lo; self.p_hi=p_hi; self.lo_={}; self.hi_={}\n",
    "    def fit(self, X, y=None):\n",
    "        Xf = pd.DataFrame(X)\n",
    "        for i in range(Xf.shape[1]):\n",
    "            s = Xf.iloc[:, i]\n",
    "            self.lo_[i] = np.nanquantile(s, self.p_lo)\n",
    "            self.hi_[i] = np.nanquantile(s, self.p_hi)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        Xf = pd.DataFrame(X).copy()\n",
    "        for i in range(Xf.shape[1]):\n",
    "            Xf.iloc[:, i] = np.clip(Xf.iloc[:, i], self.lo_[i], self.hi_[i])\n",
    "        return Xf.values\n",
    "\n",
    "numeric_pipe = Pipeline(steps=[\n",
    "    (\"impute\",  SimpleImputer(strategy=\"median\", add_indicator=True)),     # fill missing + track it   [oai_citation:5‡Appendix B Machine Learning Project Checklist.pdf](file-service://file-QnwgSaxpCdwK5LF6e2wnim)\n",
    "    (\"winsor\",  Winsorize(0.001, 0.999)),                                  # robust outlier control\n",
    "    (\"power\",   PowerTransformer(method=\"yeo-johnson\", standardize=False)), # handles skew incl. zeros\n",
    "    (\"scale\",   StandardScaler())                                          # feature scaling   [oai_citation:6‡Appendix B Machine Learning Project Checklist.pdf](file-service://file-QnwgSaxpCdwK5LF6e2wnim)\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[(\"num\", numeric_pipe, FEATURES)],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# fit on TRAIN only (no leakage)\n",
    "Xtr = df.loc[df.split==\"train\", FEATURES]\n",
    "preprocess.fit(Xtr)\n",
    "\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "joblib.dump(preprocess, \"artifacts/preprocess_v1.pkl\")\n",
    "\"Saved artifacts/preprocess_v1.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49d6d551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ((105532, 23), 0.02406852897699276),\n",
       " 'val': ((15077, 23), 0.024076407773429728),\n",
       " 'test': ((30153, 23), 0.024044042052200443)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_split(tag):\n",
    "    X = df.loc[df.split==tag, FEATURES]\n",
    "    y = df.loc[df.split==tag, TARGET].to_numpy().astype(int)\n",
    "    Xp = preprocess.transform(X)\n",
    "    # attach names for convenience\n",
    "    try:\n",
    "        cols = preprocess.get_feature_names_out()\n",
    "    except Exception:\n",
    "        cols = [f\"f{i}\" for i in range(Xp.shape[1])]\n",
    "    out_dir = ROOT/\"data/processed/features_proc\"; out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    pd.DataFrame(Xp, columns=cols).to_parquet(out_dir/f\"X_{tag}.parquet\", index=False)\n",
    "    pd.DataFrame({\"y\": y, \"kepid\": df.loc[df.split==tag, \"kepid\"].values}).to_csv(out_dir/f\"y_{tag}.csv\", index=False)\n",
    "    return Xp.shape, y.mean()\n",
    "\n",
    "shapes = {tag: transform_split(tag) for tag in [\"train\",\"val\",\"test\"]}\n",
    "shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2554fbea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generated_at                        2025-09-17T22:53:40.457712+00:00\n",
       "source_features    /Users/chrisjuarez/CPSC483_ML_Project/data/pro...\n",
       "target                                                 label_lenient\n",
       "transforms         [median+indicator, winsor 0.1%/99.9%, Yeo-John...\n",
       "files              [data/processed/features_proc/X_train.parquet,...\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "card = {\n",
    "  \"generated_at\": datetime.now(timezone.utc).isoformat(),\n",
    "  \"source_features\": str(X_PATH),\n",
    "  \"target\": TARGET,\n",
    "  \"transforms\": [\"median+indicator\", \"winsor 0.1%/99.9%\", \"Yeo-Johnson\", \"standardize\"],\n",
    "  \"files\": [\"data/processed/features_proc/X_train.parquet\",\n",
    "            \"data/processed/features_proc/X_val.parquet\",\n",
    "            \"data/processed/features_proc/X_test.parquet\",\n",
    "            \"artifacts/preprocess_v1.pkl\"]\n",
    "}\n",
    "pd.Series(card)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
